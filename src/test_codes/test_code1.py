import torch
import torch.nn as nn
import torch.nn.functional as F

ce_loss = nn.CrossEntropyLoss()
kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)

out = torch.tensor([0.3, 0.3, 0.4])
ref = torch.tensor([0.1, 0.2, 0.7])

ref = torch.tensor([[-6.9537,  -6.9670,  -7.2188,  -6.3959,  -8.1578,  -5.8776,  -7.7225, -8.0455,  -6.4596,  -6.4453,  -6.5812,  -6.0034,  -7.4468,  -6.1859, -7.5637,  -4.8260,  -6.8498,  -6.5432,  -7.6649,  -5.5409,  -7.4602, -6.2087,  -6.6483,  -8.7450,  -6.9607,  -5.4515,  -6.5753,  -5.7142, -6.2112,  -8.5310,  -7.1667,  -7.5596,  -5.5419,  -6.7210,  -7.1964, -5.9009,  -6.0151,  -5.5970,  -6.8156,  -5.5113,  -7.4623,  -5.0734, -8.1832,  -9.3864,  -7.3148,  -7.5552,  -7.5844,  -7.5713,  -9.1018, -8.7785,  -6.5935,  -6.6556,  -7.0906,  -8.2463,  -6.1028,  -5.0656, -6.1944,  -7.3645,  -6.1638,  -7.3159,  -5.7127,  -7.0194,  -7.8831, -5.1270,  -4.8733,  -6.8683,  -8.6105,  -6.0267,  -6.0806,  -6.9303, -7.9121,  -5.9534,  -7.7466,  -7.6505,  -5.6706,  -6.9306,  -4.7649, -6.0493,  -7.0037,  -7.1312,  -7.0902,  -7.0423,  -8.0262,  -5.9201, -5.5673,  -4.9930,  -5.9653,  -5.0398,  -7.5434,  -6.2794,  -5.6644, -6.3289,  -6.8779,  -7.5034,  -5.1272,  -6.9222,  -7.3881,  -5.8729, -5.2934,  -6.9484,  -7.5790,  -5.9962,  -7.5753,  -7.7883,  -7.1581, -8.5582,  -5.9097,  -4.9361,  -8.9350,  -6.9724,  -6.8514,  -5.4946, -5.7215,  -6.7725,  -5.3450,  -7.4678,  -5.3210,  -5.5864,  -7.7093, -8.1936,  -6.2660,  -5.3002,  -6.6373,  -6.4382,  -6.3056,  -6.2187, -7.8667,  -5.7931,  -7.0724,  -6.8621,  -5.6538,  -5.1337,  -7.2795, -6.0765,  -7.2254,  -5.8928,  -6.4891,  -5.5632,  -6.9563,  -5.6425, -6.6145,  -8.0679,  -5.7750,  -5.8680,  -6.6744,  -6.3735,  -8.3330, -6.3227,  -6.6562,  -5.6343,  -8.4373,  -6.0398,  -8.5099,  -6.4628, -6.6843,  -7.4580,  -6.6641,  -7.4328,  -7.0075,  -5.8344,  -8.0893, -6.1280,  -6.2733,  -8.0243,  -8.2643,  -6.5951,  -6.6453,  -6.9190, -6.4470,  -7.3050,  -6.5326,  -6.5466,  -6.7540,  -5.3827,  -8.3544, -6.2276,  -6.4215,  -6.0441,  -5.9952,  -6.6650,  -7.6259,  -6.7940, -7.8508,  -7.7437,  -6.3985,  -6.1313,  -7.7992,  -6.4724,  -7.6845, -7.8873,  -6.7375,  -6.4964,  -7.4149,  -5.0289,  -6.9671,  -6.1992, -4.8848,  -6.1278,  -5.9122,  -6.9445,  -7.0274,  -5.4634,  -7.0399, -6.3096,  -5.7528,  -7.1810,  -8.0095,  -6.4131,  -5.8822,  -7.4532, -6.9545,  -6.0351,  -7.5174,  -6.7906,  -4.8318,  -5.4447,  -6.6360, -6.0124,  -7.4439,  -7.6448,  -6.6174,  -5.7542,  -6.4514,  -6.9690, -7.4330,  -6.8751,  -6.4237,  -5.5710,  -5.6512,  -6.1709,  -5.6454, -6.1668,  -7.0289,  -6.9144,  -5.4679,  -5.8308,  -5.3893,  -6.2334, -5.7664,  -6.4602,  -5.8162,  -7.0089,  -7.2340,  -7.1248,  -6.4205, -7.6235,  -6.4192,  -5.5910,  -6.8562,  -6.8049,  -7.1759,  -6.6707, -7.1724,  -5.9173,  -5.4975,  -
                   5.4876,  -7.7127,  -6.3055,  -7.9202, -7.2745,  -5.2306,  -6.9995,  -6.6955,  -8.5420,  -6.3706,  -5.9246, -6.1368,  -5.4599,  -8.4146,  -6.3284,  -7.3363,  -5.3510,  -8.0251, -6.3793,  -9.1094,  -5.6655,  -8.4876,  -6.8160,  -5.9028,  -5.0166, -6.0917,  -5.9912,  -6.9224,  -7.6791,  -8.0967,  -6.4555,  -5.6395, -6.1897,  -7.8777,  -6.8621,  -8.6959,  -7.9164,  -6.5497,  -5.6275, -7.4659,  -6.1233,  -6.3296,  -9.1562,  -7.1999,  -6.7690,  -7.0849, -5.4469,  -6.6069,  -6.4905,  -5.0915,  -5.3728,  -7.8866,  -4.8220, -7.7613,  -5.1789,  -6.7141,  -6.9589,  -5.8993,  -5.9351,  -8.2638, -7.7928,  -5.7420,  -6.1366,  -6.3019,  -5.9403,  -7.9163,  -5.0110, -5.9301,  -5.6006,  -7.3621,  -6.6454,  -6.2298,  -7.9494,  -6.6267, -8.2077,  -8.4219,  -7.5962,  -7.5914,  -7.1645,  -7.1659,  -8.7176, -8.7927,  -3.8733,  -6.6228,  -6.3450,  -6.1876,  -5.6797,  -7.4816, -7.2596,  -7.0096,  -6.0220,  -6.1539,  -7.4933,  -7.5853,  -6.6624, -8.6108,  -5.5975,  -5.4765,  -7.3539,  -6.5238,  -8.2464,  -5.2142, -8.4561,  -5.5338,  -6.3370,  -6.2111,  -7.3330,  -7.6006,  -8.1906, -6.8987,  -6.7609,  -7.5830,  -6.2314,  -8.0721,  -5.9751,  -6.6974, -5.6842,  -6.1471,  -6.5132,  -6.6175,  -5.4554,  -6.9870,  -4.5118, -6.1606,  -4.6992,  -7.9239,  -8.0235,  -7.8453,  -6.4040,  -7.3931, -4.5903,  -7.8126,  -5.4645,  -7.7623,  -5.4330,  -7.3577,  -6.3787, -8.3756,  -6.2671,  -6.4479,  -6.9945,  -6.7066,  -5.6263,  -8.2833, -7.3538,  -6.3118,  -7.9401,  -9.6462,  -6.7721,  -6.1492,  -8.2793, -5.9749,  -6.0334,  -5.7384,  -6.3745,  -6.6086,  -6.7754,  -7.2431, -7.4419,  -6.2267,  -6.8810,  -6.4131,  -7.1064,  -6.5841,  -5.0937, -6.3476,  -5.2923,  -6.7366,  -5.3096,  -8.0666,  -5.3960,  -7.9807, -6.7057,  -6.1584,  -6.1172,  -7.1347,  -5.2631,  -6.2354,  -5.1898, -6.6639,  -7.6220,  -9.4011,  -5.8524,  -6.9226,  -6.5889,  -6.7815, -7.1788,  -6.8724,  -5.9413,  -7.6561,  -7.6006,  -6.3898,  -7.0854, -5.9471,  -6.4501,  -7.2376,  -5.6844,  -6.3162,  -7.3002,  -7.4091, -9.1078,  -7.0140,  -8.1413,  -6.0994,  -6.6547,  -6.7914,  -9.0404, -7.8660,  -4.6396,  -6.4109,  -6.8073,  -7.5145,  -6.9590,  -7.1133, -3.9155,  -5.6571,  -6.2414,  -5.5532,  -7.1613,  -7.2510,  -6.9564, -5.5624,  -5.5730,  -7.0302,  -6.5295,  -8.3867,  -6.5730,  -6.9965, -5.2163,  -6.0607,  -5.8516,  -5.3880,  -5.7420,  -7.2825,  -5.8663, -7.6770,  -6.7006,  -6.2765,  -7.0142,  -6.8604,  -5.2269,  -6.8363, -5.3638,  -6.8046,  -7.0712,  -7.5592,  -7.9637,  -8.2524,  -6.9719, -7.4164,  -6.2406, -10.2527,  -6.0323,  -7.0693,  -6.4523,  -7.1073, -4.5931]])
print(f'{ref.shape=}')
out = torch.tensor([[-7.1449, -6.8151, -7.3019, -7.3489, -9.3554, -6.1076, -6.9332, -8.1261, -6.2814, -6.2883, -7.1529, -4.7333, -8.1932, -5.8624, -6.9167, -5.3837, -5.6672, -6.0149, -8.5302, -5.8485, -7.5286, -6.9186, -6.6724, -8.4577, -6.4220, -4.4172, -6.1646, -5.8870, -5.9169, -7.3208, -7.3908, -6.3415, -6.6666, -7.3532, -7.4701, -6.3466, -6.2948, -5.7991, -6.1175, -6.1564, -7.3333, -5.8284, -8.2079, -9.2858, -7.1493, -6.6700, -8.7974, -7.7583, -8.3407, -8.4942, -6.6359, -7.4179, -7.0787, -7.3891, -6.8443, -5.7267, -6.4658, -7.3076, -5.6581, -5.9382, -5.3143, -7.0162, -7.1387, -5.6409, -6.4513, -7.2805, -8.2540, -5.2499, -6.6222, -7.5144, -7.9878, -6.6468, -7.2839, -7.5810, -5.0834, -6.6608, -5.8093, -6.2875, -5.6390, -5.1339, -7.4139, -6.6248, -7.2008, -6.5867, -4.6527, -6.7302, -5.5838, -4.9849, -7.3041, -6.8039, -5.5744, -6.3799, -5.9147, -7.0213, -5.6132, -6.0133, -7.6465, -5.7707, -5.7159, -6.2883, -7.9412, -6.3421, -7.2904, -7.6278, -8.2084, -8.3908, -6.8549, -5.1645, -8.8335, -7.3874, -7.2426, -6.8151, -5.1779, -6.7724, -6.2324, -6.5865, -6.3275, -5.8326, -7.0595, -7.7309, -6.6234, -5.4566, -6.2078, -7.7143, -6.6309, -6.4407, -8.3276, -5.5809, -7.5566, -7.6709, -5.6858, -5.7506, -7.2452, -6.0453, -5.9620, -6.8656, -6.6209, -5.0769, -7.0996, -5.4786, -5.8263, -7.5379, -5.8834, -6.1129, -6.6974, -6.6315, -9.0360, -7.0781, -5.5088, -6.5077, -8.7243, -5.6683, -8.5269, -6.5525, -6.5093, -6.7233, -7.1603, -7.9378, -8.0543, -5.2793, -8.0704, -6.5409, -7.7377, -8.0765, -8.6689, -5.9309, -7.5172, -6.5367, -7.3784, -8.8897, -6.8946, -6.4152, -5.6349, -5.0700, -8.1521, -6.2578, -6.3093, -6.1822, -6.4851, -5.8524, -7.1585, -8.0975, -6.6848, -8.2482, -6.2353, -5.9902, -7.1337, -5.5674, -6.5313, -7.2986, -6.4183, -6.8474, -6.1340, -4.9874, -6.5588, -5.8076, -5.5594, -5.2973, -6.3069, -6.5619, -7.7492, -6.2124, -6.2498, -6.7954, -7.0403, -6.9122, -7.1419, -6.7259, -6.9991, -8.2675, -7.2777, -6.1773, -7.1205, -7.6659, -5.8532, -7.6368, -6.7792, -5.9104, -7.4440, -7.6206, -6.1678, -6.1321, -5.7131, -6.4756, -8.0842, -7.0215, -6.0544, -5.5110, -4.9890, -5.5013, -6.2615, -5.8230, -5.9146, -7.0979, -6.0767, -5.9127, -5.6680, -5.8385, -5.7881, -6.4061, -5.9625, -6.8992, -7.0822, -6.3263, -6.5143, -7.5267, -6.7720, -5.4769, -6.9580, -6.1263, -7.8816, -7.0498, -7.5517, -5.4531, -5.0792, -
                   6.1436, -8.0572, -6.6770, -6.8688, -8.4684, -4.6258, -6.8375, -6.6328, -7.7941, -5.6475, -5.0130, -7.2836, -6.5912, -8.0050, -6.1323, -6.9645, -6.3129, -7.9492, -5.6364, -8.1509, -7.0554, -7.9969, -8.5019, -6.3298, -6.1584, -6.5706, -5.4484, -7.4111, -7.8754, -8.2429, -6.5214, -6.2141, -5.9885, -8.2848, -7.4884, -7.4906, -7.7145, -6.2310, -5.1463, -6.8858, -5.9488, -6.2307, -8.2997, -5.8396, -6.4126, -7.3869, -5.8794, -6.6814, -7.8542, -6.2037, -5.6769, -7.3519, -4.9913, -7.2005, -5.4361, -6.1086, -6.9763, -5.9239, -5.9975, -8.0024, -7.0857, -4.9711, -5.6689, -6.7151, -6.2178, -8.0559, -5.0735, -6.4308, -6.7598, -6.1624, -5.5407, -6.7687, -7.2206, -5.9319, -8.2298, -7.7925, -7.5446, -6.8666, -5.8340, -7.3339, -7.4405, -8.2237, -3.5641, -6.5949, -8.0006, -5.5827, -6.1899, -7.4891, -8.0804, -5.4161, -5.3590, -6.7151, -7.3134, -7.4064, -7.4664, -8.9152, -5.5260, -6.0517, -8.0919, -6.3758, -7.6372, -5.7013, -8.2469, -5.1599, -5.9077, -6.0697, -6.5261, -6.8829, -7.3204, -8.0717, -6.9605, -6.8396, -6.6148, -7.4000, -5.9022, -7.1797, -5.8262, -6.2624, -5.3799, -6.5167, -7.0551, -6.5297, -5.8407, -6.2538, -4.8846, -6.9572, -7.3398, -7.4725, -5.2078, -7.7749, -5.9503, -9.1277, -5.1599, -8.3786, -5.6735, -8.7397, -6.2439, -7.4978, -5.9223, -5.8646, -6.8652, -6.4596, -5.8280, -7.9363, -7.7199, -6.3680, -7.8701, -9.4314, -7.3066, -5.8715, -7.2972, -6.5896, -5.9321, -5.8259, -6.2239, -6.1540, -5.7041, -7.5322, -8.2001, -5.4615, -6.1132, -7.6849, -7.5050, -7.4154, -6.7103, -6.7559, -5.2635, -6.4407, -5.5691, -7.1253, -5.7596, -6.9888, -7.0474, -8.0409, -7.5973, -7.5047, -6.1672, -7.0014, -4.4326, -7.4472, -8.5510, -7.9917, -5.3651, -7.0259, -5.7318, -5.8385, -6.3522, -6.4002, -6.6898, -8.0509, -7.4429, -6.9611, -7.1320, -6.3565, -5.7868, -7.2416, -5.6078, -6.1426, -7.2491, -7.7540, -9.4405, -7.8228, -8.2654, -6.9016, -6.6204, -6.5666, -8.7760, -7.3527, -4.8103, -7.1470, -6.4220, -7.8947, -7.2243, -5.8133, -4.6495, -4.6682, -7.2008, -5.1679, -7.0890, -7.9059, -5.9654, -6.4542, -5.2345, -6.6759, -6.5988, -7.6648, -6.1733, -6.5314, -5.4738, -5.5406, -5.9841, -5.4582, -5.4945, -8.1367, -5.4064, -7.5857, -6.6744, -6.8729, -6.7793, -7.3868, -4.6117, -6.6783, -5.7646, -5.2348, -6.8944, -8.1466, -7.2826, -7.8180, -6.7244, -8.0587, -6.7288, -9.4167, -5.0785, -7.1838, -5.8047, -7.0979, -5.2775]])
print(f'{out.shape=}')


out = F.log_softmax(out, dim=1)
ref = F.log_softmax(ref, dim=1)


loss_score = kl_loss(out, ref)
print(loss_score)


loss_score = kl_loss(ref, ref)
print(loss_score)